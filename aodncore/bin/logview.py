#!/usr/bin/env python

"""
Script to parse and view logs generated by pipelines.
"""

import argparse
from collections import OrderedDict
import re
import sys


# location of logs
LOGDIR_BASE = '/sw/chef/src/tmp/p2_logs'
LOG_WATCH = LOGDIR_BASE + '/watchservice/pipeline_watchservice-stderr.log'
LOGDIR_CELERY = LOGDIR_BASE + '/celery'
LOGDIR_PROCESS = LOGDIR_BASE + '/process'

# regular expressions to match log format and define fields extracted from log
LOG_FIELDS = OrderedDict([
    ('time', r"(?P<time>\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2},\d{3})\s+"),
    ('level', r"(?P<level>[A-Z]+)\s+"),
    ('task_name', r"tasks.(?P<task_name>\w+)"),
    ('task_id', r"\[(?P<task_id>[0-9a-f-]+)\]\s+"),
    ('message', r"(?P<message>.*)")
])
INPUT_REGEX = re.compile(''.join(LOG_FIELDS.values()))
OUTPUT_FORMAT = '{time:20} {level:>9} {message}\n'


def find_log(input_file):
    """
    Given the name of an uploaded file, find the log file from the pipeline process that handled it.

    :param str input_file: Name of uploaded file
    :return: Full path to log file.
    """
    # TODO: implement find_log
    # Things to try:
    #   Read LOG_WATCH file and find the file name
    #   Read all process logs in LOGDIR_PROCESS and use pattern match
    return ''


def parse_args():
    """Parse the command line"""
    parser = argparse.ArgumentParser()
    parser.add_argument('logfile', help='pipeline task log file')
    parser.add_argument('-t', '--task_id', help='lines for task_id', metavar='ID')
    parser.add_argument('-e', '--errors', help='error lines only', action='store_true')
    parser.add_argument('-w', '--warnings', help='warning & error lines only', action='store_true')
    parser.add_argument('-p', '--pattern', help='lines matching regex pattern', metavar='REGEX')
    parser.add_argument('-f', '--file', help='name of processed file')

    args = parser.parse_args()

    print('Args: {}\n'.format(args))

    if args.file:
        args.logfile = find_log(args.file)

    return args


def parse_log(logfile):
    """
    Parse the log returning a list of log data. Each list item is a dictionary of fieldname: value pairs.

    :param str logfile: Path to log file
    :return: list of log details from each line.

    """
    # read all log lines
    with open(logfile) as log:
        lines = log.readlines()

    # parse each line
    log_data = []
    for line in lines:
        line = line.strip()

        m = INPUT_REGEX.match(line)
        if m is not None:
            log_data.append(m.groupdict())

        # TODO: deal with unformatted lines

    return log_data


def print_output(log_data, fmt=OUTPUT_FORMAT):
    """
    Print the log data to stdout.

    :param list log_data: List of log data as returned by parse_log
    :param str fmt: format string to convert log fields into a line of output

    """
    output = []
    for data in log_data:
        output.append(fmt.format(**data))
        # TODO: wrap long lines?

    try:
        sys.stdout.writelines(output)
        sys.stdout.flush()
    except IOError:
        # this can happen if output is piped to `head` or `less`
        pass


def view_log(logfile, task_id=None, errors=False, warnings=False, pattern=None):

    all_data = parse_log(logfile)

    levels = None
    if errors:
        levels = ('ERROR', 'CRITICAL')
    if warnings:
        levels = ('WARNING', 'ERROR', 'CRITICAL')

    if pattern:
        pattern = re.compile(args.pattern)

    out_data = []
    for data in all_data:
        if task_id and data['task_id'] != task_id:
            continue
        if levels and data['level'] not in levels:
            continue
        if pattern and not pattern.search(data['message']):
            continue
        # TODO: filter by handler step?

        out_data.append(data)

    # format & print output
    print_output(out_data)


if __name__ == '__main__':
    args = parse_args()

    # TODO: filter by file name (parent or child)

    view_log(args.logfile, task_id=args.task_id, errors=args.errors, warnings=args.warnings, pattern=args.pattern)

    exit(0)
